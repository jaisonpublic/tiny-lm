# Speed run training config: ~500 steps â‰ˆ 1 hour on RTX 5070
# total batch size = batch_size * seq_len * accumulate_grad_batches * num_devices
# example: 128 * 256 * 1 * 1 = 32768 tokens per update
# ~16.4M tokens total in 500 steps

# Precision and loss masking
precision: "bf16-mixed"
ignore_index: -100

# Optimizer and training stability
weight_decay: 0.1
betas: [0.9, 0.95]
eps: 1.0e-8
grad_clip_norm: 1.0

# Learning-rate schedule (slightly higher LR for small model)
learning_rate: 6.0e-4
scheduler: "cosine"
warmup_ratio: 0.02
min_lr: 6.0e-5

# Tokens, steps, and batching
accumulate_grad_batches: 1
max_steps: 500
val_every_n_steps: 50
system_metrics_every_n_steps: 5

# Checkpointing
save_every_n_steps: 100
resume_from_checkpoint: null
